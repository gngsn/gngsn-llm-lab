{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Tutorial\n",
    "\n",
    "\n",
    "## Concept.\n",
    "\n",
    "### LLM\n",
    "Large Language Models are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, question answering, and more, without needing task-specific fine tuning for every scenario.\n",
    "\n",
    "Modern LLMs are typically accessed through a chat model interface that takes a list of [messages](https://python.langchain.com/docs/concepts/messages/) as input and returns a message as output.\n",
    "\n",
    "\n",
    "### RAG\n",
    "Retrieval-Augmented Generation. \n",
    "It’s like giving your chatbot a brain full of **searchable knowledge**.\n",
    "Imagine a chatbot that could tap into a vast library of information and generate creative text. That’s the magic of RAG.\n",
    "\n",
    "### LangChain\n",
    "\n",
    "LangChain is a powerful Python-based framework designed to simplify the development of applications powered by large language models (LLMs). It does this by providing a modular and flexible structure that streamlines common NLP tasks and the integration of various AI components.\n",
    "\n",
    "#### Key Players\n",
    "\n",
    "- `Chains`: These are essentially pipelines that connect various components within the chatbot architecture. In our case, we’ll construct a chain that seamlessly integrates the user’s query with the retrieval manager (RAG) and subsequently, the response generation stage.\n",
    "- `Agents`: These are like the workers in your chatbot factory. We will create a specialized agent that acts as a bridge between LangChain and RAG. It receives the user’s query from the chain, transmits it to RAG for information retrieval, and then feeds the retrieved data back into the chain for further processing.\n",
    "- `Prompts`: LangChain empowers you to design prompts that guide the LLM in crafting the most effective response. For example, a prompt might instruct the LLM to provide a succinct answer to the user’s question or generate a concise summary of a retrieved Wikipedia article.\n",
    "\n",
    "\n",
    "#### LifeCycle\n",
    "\n",
    "- `Development`: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\n",
    "- `Productionization`: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\n",
    "- `Deployment`: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\n",
    "\n",
    "\n",
    "#### 🤝🏻 RAG\n",
    "The synergy between RAG’s information retrieval capabilities and LangChain’s modular structure lays the foundation for constructing a chatbot that leverages the vast and complicated content in any data source (in our case PDF) to deliver informative and creative responses to user queries.\n",
    "\n",
    "\n",
    "### ChatModels\n",
    "\n",
    "[docs](https://python.langchain.com/docs/concepts/chat_models/)\n",
    "Chat models offer a standard set of parameters that can be used to configure the model. These parameters are typically used to control the behavior of the model, such as the temperature of the output, the maximum number of tokens in the response, and the maximum time to wait for a response. Please see the standard parameters section for more details.\n",
    "\n",
    "\n",
    "#### Interface\n",
    "\n",
    "[BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html)\n",
    "\n",
    "**Key methods**\n",
    "\n",
    "- `invoke`: The primary method for interacting with a chat model. It takes a list of messages as input and returns a list of messages as output.\n",
    "- `stream`: A method that allows you to stream the output of a chat model as it is generated.\n",
    "- `batch`: A method that allows you to batch multiple requests to a chat model together for more efficient processing.\n",
    "- `bind_tools`: A method that allows you to bind a tool to a chat model for use in the model's execution context.\n",
    "- `with_structured_output`: A wrapper around the invoke method for models that natively support structured output.\n",
    "\n",
    "\n",
    "\n",
    "## 1. Setting Environment \n",
    "\n",
    "### 1.1 `pipx` installation\n",
    "### 1.2 `poetry` installation\n",
    "\n",
    "`poetry` 명령어가 인식되지 않는다면, poetry 설치 경로 `$PATH` 에 추가\n",
    "\n",
    "```bash\n",
    "export PATH=\"$HOME/.local/bin:$PATH\"\n",
    "``` \n",
    "\n",
    "## 2. API Keys\n",
    "\n",
    "Visit '[Get a Gemini API key](https://ai.google.dev/gemini-api/docs/api-key)' to create an account if you don’t have one.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 3. create a poetry project\n",
    "\n",
    "poetry를 사용한 프로젝트 생성 명령어는 다음과 같다.\n",
    "\n",
    "```bash\n",
    "poetry new poetry-demo\n",
    "```\n",
    "이 명령어는 poetry-demo라는 디렉토리를 만들어주고, 해당 디렉토리는 아래와 같은 구조를 가진다.\n",
    "\n",
    "```\n",
    "poetry-demo\n",
    "├── pyproject.toml\n",
    "├── README.rst\n",
    "├── poetry_demo\n",
    "│   └── __init__.py\n",
    "└── tests\n",
    "    ├── __init__.py\n",
    "    └── test_poetry_demo.py\n",
    "```\n",
    "\n",
    "**✔️ `pyproject.toml`**\n",
    "toml 파일은 중요 요소 중 하나인데, 프로젝트의 의존성을 조율해 주는 파일이기 때문.\n",
    "`pyproject.toml` 파일을 변경할 필요 없이 아래 command를 통해 수정이 가능. \n",
    "\n",
    "```bash\n",
    "poetry add some-dependencies-you-want\n",
    "```\n",
    "\n",
    "`poetry.lock`\n",
    "항상 같은 의존성 환경에서 개발할 수 있도록 의존성 version locked.\n",
    "처음 프로젝트 구성 시, 프로젝트에 정의된 의존성 파일들을 설치를 위한 명령어.\n",
    "\n",
    "```\n",
    "poetry install\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "## Build a ChatBot Application!\n",
    "\n",
    "### Install Dependencies\n",
    "\n",
    "```\n",
    "poetry add langchain openai pinecone-client langchain-pinecone langchain-openai python-dotenv pypdf\n",
    "```\n",
    "\n",
    "### Set Environment Variables\n",
    "\n",
    "Create a file named `.env` in your project directory. Remember to keep this file safe — it’ll hold sensitive information like your API keys. We’ll use a Python library called dotenv to access these keys securely.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY={openAI API key}\n",
    "INDEX_NAME={Pinecone index name}\n",
    "PINECONE_API_KEY={Pinecone API key}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2. Loading the PDF Content\n",
    "\n",
    "[langchain - document_loaders](https://python.langchain.com/docs/integrations/document_loaders/pypdfloader/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "\n",
    "def get_working_dir():\n",
    "    working_dir = \"gngsn-llm-lab\"\n",
    "    root = []\n",
    "    for p in os.getcwd().split(\"/\"):\n",
    "        root.append(p)\n",
    "        if (p == working_dir):\n",
    "            break\n",
    "    return \"/\".join(root)\n",
    "\n",
    "\n",
    "root_dir = get_working_dir()\n",
    "loader = PyPDFLoader(root_dir + \"/data/오가노이드사이언스_증권신고서.pdf\")\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pp(document[0].metadata)\n",
    "print(document[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3. Splitting Documents into Chunks\n",
    "\n",
    "**Understanding Chunk Size and Overlap**\n",
    "\n",
    "The CharacterTextSplitter takes two important parameters:\n",
    "\n",
    "- `chunk_size`: This defines the maximum number of characters in each split chunk. A smaller chunk size creates more, but shorter, chunks. A larger chunk size creates fewer, but longer, chunks. It’s crucial to find a good balance when choosing the chunk_size. Extremely small chunks might not contain enough context for the language model to understand properly, whereas very large chunks might become processing bottlenecks.\n",
    "\n",
    "- `chunk_overlap`: This specifies the number of characters that overlap between consecutive chunks. Overlap helps the language model maintain context between neighboring pieces, especially important for longer sentences that might be split across chunks. We set chunk_overlap to 100 characters. This ensures some overlap between chunks, helping the language model understand the flow of information across splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(document)\n",
    "print(f\"created {len(texts)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding models\n",
    "\n",
    "[LangChain - Embedding models](https://python.langchain.com/docs/concepts/embedding_models/)\n",
    "\n",
    "<img src='./img/figure1-1.png' />\n",
    "\n",
    "- (1) **Embed text as a vector**: Embeddings transform text into a numerical vector representation.\n",
    "- (2) **Measure similarity**: Embedding vectors can be compared using simple mathematical operations.\n",
    "\n",
    "[AI-Powered (Vector) Search](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "embeddings = VertexAIEmbeddings(model=\"text-embedding-004\")\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Initializing OpenAI**: We initialize the ChatOpenAI LLM specifying temperature=0. This encourages the model to generate more focused and less creative responses.\n",
    "- **Core RAG Chain**: In LangChain, RetrievalQA.from_chain_type is a function used to create a RetrievalQA chain, a specific type of chain designed for question answering tasks.\n",
    "\n",
    "\n",
    "If you want customized answers but no training, use RAG.\n",
    "If you want a chatbot that improves over time, use RLHF.\n",
    "If you want full customization, fine-tune your own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "chat = init_chat_model(\"gemini-2.0-flash-001\", model_provider=\"google_vertexai\")\n",
    "# chat = ChatOpenAI(verbose=True, temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=chat, chain_type=\"stuff\", retriever=vector_store.as_retriever()\n",
    ")\n",
    "\n",
    "res = qa.invoke(\"What are the applications of generative AI according the the paper? Please number each application.\")\n",
    "print(res)\n",
    "\n",
    "res = qa.invoke(\"Can you please elaborate more on application number 2?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the document referring to? Please summarize the whole content.\n",
      "Bot: This document outlines regulations and potential restrictions related to participating in the demand forecast for a public offering (IPO).\n",
      "\n",
      "Here's a summary of the key points:\n",
      "\n",
      "*   **Restrictions on Participation:** Certain entities are restricted from participating in the demand forecast, including those with conflicts of interest, major shareholders of companies recently underwritten by the lead manager, those currently classified as \"unfaithful demand forecast participants,\" and certain venture capital firms.\n",
      "*   **\"Unfaithful Demand Forecast Participant\" Designation:** If an institution is designated as an \"unfaithful demand forecast participant\" after participating in the current demand forecast, they will be restricted from participating in future demand forecasts and from being allocated IPO shares for a certain period.\n",
      "*   **Regulations on IPO Pricing:** The document references regulations regarding how the IPO price is determined, including the possibility of allowing certain venture capital firms to participate in the demand forecast.\n",
      "*   **Put Option (환매청구권):** If certain venture capital firms are allowed to participate in the demand forecast, general investors may be granted the right to sell their shares back to the underwriter under certain conditions.\n",
      "*   **Disqualification as \"Unfaithful Demand Forecast Participant\":** The document warns that institutions may be classified as \"unfaithful demand forecast participants\" based on specific reasons outlined in the \"Securities Underwriting Business Regulations.\" Institutions classified as \"unfaithful demand forecast participants\" are restricted from participating in future demand forecasts led by Korea Investment & Securities.\n"
     ]
    }
   ],
   "source": [
    "### 🤖 Create a Simple Chatbot\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "chat = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "loader = PyPDFLoader(root_dir + \"/data/오가노이드사이언스_증권신고서.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents, GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\"))\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "qa = RetrievalQA.from_chain_type(llm=chat, retriever=retriever)\n",
    "\n",
    "print(\"Question: What is the document referring to? Please summarize the whole content.\")\n",
    "response = qa.run(\"What is the document referring to? Please summarize the whole content\")\n",
    "print(\"Bot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8t/1y7tw5tx0f5gz3rvdmvjbnth0000gn/T/ipykernel_53143/4199014866.py:17: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa({\"question\": q, \"chat_history\": chat_history})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 입력된 문서에 대해 한글로 요약 정리해주세요.\n",
      "Bot: {'question': '입력된 문서에 대해 한글로 요약 정리해주세요.', 'chat_history': [('입력된 문서에 대해 한글로 요약 정리해주세요.', '제공된 문서는 편도 오가노이드가 SARS-CoV-2 감염 시 인간 감염을 상세히 재현할 수 있는 최적화된 연구 플랫폼이라고 설명합니다. 이는 미지의 감염체를 신속하게 식별하고 새로운 치료제의 효과를 평가하는 데 유용합니다. 기존의 세포나 동물 모델이 인간 바이러스 감염을 제대로 재현하지 못하는 반면, 편도 오가노이드는 인체와 유사한 감염 환경을 제공하여 차세대 바이러스 감염 연구 및 약물 평가 플랫폼으로 기대됩니다.')], 'answer': '제공된 문서는 편도 오가노이드가 SARS-CoV-2 감염 시 인간 감염을 상세히 재현할 수 있는 최적화된 연구 플랫폼이라고 설명합니다. 이는 미지의 감염체를 신속하게 식별하고 새로운 치료제의 효과를 평가하는 데 유용합니다. 기존의 세포나 동물 모델이 인간 바이러스 감염을 제대로 재현하지 못하는 반면, 편도 오가노이드는 인체와 유사한 감염 환경을 제공하여 차세대 바이러스 감염 연구 및 약물 평가 플랫폼으로 기대됩니다.'}\n",
      "Question: 오가노이드의 2024년 11월 매출액이 얼마야?\n",
      "Bot: {'question': '오가노이드의 2024년 11월 매출액이 얼마야?', 'chat_history': [('입력된 문서에 대해 한글로 요약 정리해주세요.', '제공된 문서는 편도 오가노이드가 SARS-CoV-2 감염 시 인간 감염을 상세히 재현할 수 있는 최적화된 연구 플랫폼이라고 설명합니다. 이는 미지의 감염체를 신속하게 식별하고 새로운 치료제의 효과를 평가하는 데 유용합니다. 기존의 세포나 동물 모델이 인간 바이러스 감염을 제대로 재현하지 못하는 반면, 편도 오가노이드는 인체와 유사한 감염 환경을 제공하여 차세대 바이러스 감염 연구 및 약물 평가 플랫폼으로 기대됩니다.'), ('오가노이드의 2024년 11월 매출액이 얼마야?', '오가노이드의 2024년 11월 매출액은 1,396백만원입니다.')], 'answer': '오가노이드의 2024년 11월 매출액은 1,396백만원입니다.'}\n",
      "Question: 오가노이드의 2024년 3분기 순이익은 얼마야?\n",
      "Bot: {'question': '오가노이드의 2024년 3분기 순이익은 얼마야?', 'chat_history': [('입력된 문서에 대해 한글로 요약 정리해주세요.', '제공된 문서는 편도 오가노이드가 SARS-CoV-2 감염 시 인간 감염을 상세히 재현할 수 있는 최적화된 연구 플랫폼이라고 설명합니다. 이는 미지의 감염체를 신속하게 식별하고 새로운 치료제의 효과를 평가하는 데 유용합니다. 기존의 세포나 동물 모델이 인간 바이러스 감염을 제대로 재현하지 못하는 반면, 편도 오가노이드는 인체와 유사한 감염 환경을 제공하여 차세대 바이러스 감염 연구 및 약물 평가 플랫폼으로 기대됩니다.'), ('오가노이드의 2024년 11월 매출액이 얼마야?', '오가노이드의 2024년 11월 매출액은 1,396백만원입니다.'), ('오가노이드의 2024년 3분기 순이익은 얼마야?', '제공된 정보에 따르면 오가노이드 재생치료제의 기술이전으로 인한 자금이 유입되기 전까지 신소재평가솔루션과 재생치료제의 첨단재생의료, 조건부상용화 매출 금액 등으로 연구개발비용을 소요할 것으로 예상됩니다. 그러나 2024년 3분기 순이익에 대한 구체적인 정보는 제공되지 않았습니다.')], 'answer': '제공된 정보에 따르면 오가노이드 재생치료제의 기술이전으로 인한 자금이 유입되기 전까지 신소재평가솔루션과 재생치료제의 첨단재생의료, 조건부상용화 매출 금액 등으로 연구개발비용을 소요할 것으로 예상됩니다. 그러나 2024년 3분기 순이익에 대한 구체적인 정보는 제공되지 않았습니다.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=chat, chain_type=\"stuff\", retriever=retriever\n",
    ")\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "questions = [\n",
    "    \"입력된 문서에 대해 한글로 요약 정리해주세요.\",\n",
    "    \"오가노이드의 2024년 11월 매출액이 얼마야?\",\n",
    "    \"오가노이드의 2024년 3분기 순이익은 얼마야?\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    response = qa({\"question\": q, \"chat_history\": chat_history})\n",
    "\n",
    "    history = (response[\"question\"], response[\"answer\"])\n",
    "    chat_history.append(history)\n",
    "\n",
    "    print(\"Question:\", q)\n",
    "    print(\"Bot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 입력된 문서에 대해 한글로 요약 정리해주세요.\n",
      "Bot: {'question': '입력된 문서에 대해 한글로 요약 정리해주세요.', 'chat_history': [('입력된 문서에 대해 한글로 요약 정리해주세요.', '해당 회사의 연구에 따르면 편도 오가노이드는 SARS-CoV-2 감염 시 인체 감염을 상세히 재현하는 최적화된 연구 플랫폼입니다. 이는 미지의 감염체를 신속하게 동정하거나 새로운 치료제의 효능을 평가하는 데 유용합니다. 기존 세포/동물 모델이 인간 바이러스 감염 및 증식에 어려움을 겪는 반면, 편도 오가노이드는 인체와 유사한 감염 환경을 구축하여 차세대 바이러스 감염 연구 및 약물 평가 플랫폼으로 기대됩니다.')], 'answer': '해당 회사의 연구에 따르면 편도 오가노이드는 SARS-CoV-2 감염 시 인체 감염을 상세히 재현하는 최적화된 연구 플랫폼입니다. 이는 미지의 감염체를 신속하게 동정하거나 새로운 치료제의 효능을 평가하는 데 유용합니다. 기존 세포/동물 모델이 인간 바이러스 감염 및 증식에 어려움을 겪는 반면, 편도 오가노이드는 인체와 유사한 감염 환경을 구축하여 차세대 바이러스 감염 연구 및 약물 평가 플랫폼으로 기대됩니다.'}\n",
      "Question: 오가노이드의 2024년 11월 매출액이 얼마야?\n",
      "Bot: {'question': '오가노이드의 2024년 11월 매출액이 얼마야?', 'chat_history': [('입력된 문서에 대해 한글로 요약 정리해주세요.', '해당 회사의 연구에 따르면 편도 오가노이드는 SARS-CoV-2 감염 시 인체 감염을 상세히 재현하는 최적화된 연구 플랫폼입니다. 이는 미지의 감염체를 신속하게 동정하거나 새로운 치료제의 효능을 평가하는 데 유용합니다. 기존 세포/동물 모델이 인간 바이러스 감염 및 증식에 어려움을 겪는 반면, 편도 오가노이드는 인체와 유사한 감염 환경을 구축하여 차세대 바이러스 감염 연구 및 약물 평가 플랫폼으로 기대됩니다.'), ('오가노이드의 2024년 11월 매출액이 얼마야?', '2024년 11월 매출액은 1,396백만원입니다.')], 'answer': '2024년 11월 매출액은 1,396백만원입니다.'}\n",
      "Question: 오가노이드의 2024년 3분기 순이익은 얼마야?\n",
      "Bot: {'question': '오가노이드의 2024년 3분기 순이익은 얼마야?', 'chat_history': [('입력된 문서에 대해 한글로 요약 정리해주세요.', '해당 회사의 연구에 따르면 편도 오가노이드는 SARS-CoV-2 감염 시 인체 감염을 상세히 재현하는 최적화된 연구 플랫폼입니다. 이는 미지의 감염체를 신속하게 동정하거나 새로운 치료제의 효능을 평가하는 데 유용합니다. 기존 세포/동물 모델이 인간 바이러스 감염 및 증식에 어려움을 겪는 반면, 편도 오가노이드는 인체와 유사한 감염 환경을 구축하여 차세대 바이러스 감염 연구 및 약물 평가 플랫폼으로 기대됩니다.'), ('오가노이드의 2024년 11월 매출액이 얼마야?', '2024년 11월 매출액은 1,396백만원입니다.'), ('오가노이드의 2024년 3분기 순이익은 얼마야?', \"I am sorry, but the provided context does not contain information about the company's net profit. Therefore, I cannot answer your question.\")], 'answer': \"I am sorry, but the provided context does not contain information about the company's net profit. Therefore, I cannot answer your question.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(\"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\")\n",
    "\n",
    "ANSWER_PROMPT_TEMPLATE = PromptTemplate.from_template(\"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\")\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=chat, chain_type=\"stuff\", retriever=retriever,\n",
    "    condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "    combine_docs_chain_kwargs={\"prompt\": ANSWER_PROMPT_TEMPLATE},\n",
    ")\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "questions = [\n",
    "    \"입력된 문서에 대해 한글로 요약 정리해주세요.\",\n",
    "    \"오가노이드의 2024년 11월 매출액이 얼마야?\",\n",
    "    \"오가노이드의 2024년 3분기 순이익은 얼마야?\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    response = qa({\"question\": q, \"chat_history\": chat_history})\n",
    "\n",
    "    history = (response[\"question\"], response[\"answer\"])\n",
    "    chat_history.append(history)\n",
    "\n",
    "    print(\"Question:\", q)\n",
    "    print(\"Bot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ref\n",
    "- https://aistudio.google.com/app/apikey\n",
    "- https://medium.com/@suraj_bansal/build-your-own-ai-chatbot-a-beginners-guide-to-rag-and-langchain-0189a18ec401\n",
    "- https://wikidocs.net/book/14473"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
