{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Tutorial\n",
    "\n",
    "\n",
    "## Concept.\n",
    "\n",
    "### LLM\n",
    "Large Language Models are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, question answering, and more, without needing task-specific fine tuning for every scenario.\n",
    "\n",
    "Modern LLMs are typically accessed through a chat model interface that takes a list of [messages](https://python.langchain.com/docs/concepts/messages/) as input and returns a message as output.\n",
    "\n",
    "\n",
    "### RAG\n",
    "Retrieval-Augmented Generation. \n",
    "Itâ€™s like giving your chatbot a brain full of **searchable knowledge**.\n",
    "Imagine a chatbot that could tap into a vast library of information and generate creative text. Thatâ€™s the magic of RAG.\n",
    "\n",
    "### LangChain\n",
    "\n",
    "LangChain is a powerful Python-based framework designed to simplify the development of applications powered by large language models (LLMs). It does this by providing a modular and flexible structure that streamlines common NLP tasks and the integration of various AI components.\n",
    "\n",
    "#### Key Players\n",
    "\n",
    "- `Chains`: These are essentially pipelines that connect various components within the chatbot architecture. In our case, weâ€™ll construct a chain that seamlessly integrates the userâ€™s query with the retrieval manager (RAG) and subsequently, the response generation stage.\n",
    "- `Agents`: These are like the workers in your chatbot factory. We will create a specialized agent that acts as a bridge between LangChain and RAG. It receives the userâ€™s query from the chain, transmits it to RAG for information retrieval, and then feeds the retrieved data back into the chain for further processing.\n",
    "- `Prompts`: LangChain empowers you to design prompts that guide the LLM in crafting the most effective response. For example, a prompt might instruct the LLM to provide a succinct answer to the userâ€™s question or generate a concise summary of a retrieved Wikipedia article.\n",
    "\n",
    "\n",
    "#### LifeCycle\n",
    "\n",
    "- `Development`: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\n",
    "- `Productionization`: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\n",
    "- `Deployment`: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\n",
    "\n",
    "\n",
    "#### ğŸ¤ğŸ» RAG\n",
    "The synergy between RAGâ€™s information retrieval capabilities and LangChainâ€™s modular structure lays the foundation for constructing a chatbot that leverages the vast and complicated content in any data source (in our case PDF) to deliver informative and creative responses to user queries.\n",
    "\n",
    "\n",
    "### ChatModels\n",
    "\n",
    "[docs](https://python.langchain.com/docs/concepts/chat_models/)\n",
    "Chat models offer a standard set of parameters that can be used to configure the model. These parameters are typically used to control the behavior of the model, such as the temperature of the output, the maximum number of tokens in the response, and the maximum time to wait for a response. Please see the standard parameters section for more details.\n",
    "\n",
    "\n",
    "#### Interface\n",
    "\n",
    "[BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html)\n",
    "\n",
    "**Key methods**\n",
    "\n",
    "- `invoke`: The primary method for interacting with a chat model. It takes a list of messages as input and returns a list of messages as output.\n",
    "- `stream`: A method that allows you to stream the output of a chat model as it is generated.\n",
    "- `batch`: A method that allows you to batch multiple requests to a chat model together for more efficient processing.\n",
    "- `bind_tools`: A method that allows you to bind a tool to a chat model for use in the model's execution context.\n",
    "- `with_structured_output`: A wrapper around the invoke method for models that natively support structured output.\n",
    "\n",
    "\n",
    "\n",
    "## 1. Setting Environment \n",
    "\n",
    "### 1.1 `pipx` installation\n",
    "### 1.2 `poetry` installation\n",
    "\n",
    "`poetry` ëª…ë ¹ì–´ê°€ ì¸ì‹ë˜ì§€ ì•ŠëŠ”ë‹¤ë©´, poetry ì„¤ì¹˜ ê²½ë¡œ `$PATH` ì— ì¶”ê°€\n",
    "\n",
    "```bash\n",
    "export PATH=\"$HOME/.local/bin:$PATH\"\n",
    "``` \n",
    "\n",
    "## 2. API Keys\n",
    "\n",
    "Visit '[Get a Gemini API key](https://ai.google.dev/gemini-api/docs/api-key)' to create an account if you donâ€™t have one.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 3. create a poetry project\n",
    "\n",
    "poetryë¥¼ ì‚¬ìš©í•œ í”„ë¡œì íŠ¸ ìƒì„± ëª…ë ¹ì–´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n",
    "\n",
    "```bash\n",
    "poetry new poetry-demo\n",
    "```\n",
    "ì´ ëª…ë ¹ì–´ëŠ” poetry-demoë¼ëŠ” ë””ë ‰í† ë¦¬ë¥¼ ë§Œë“¤ì–´ì£¼ê³ , í•´ë‹¹ ë””ë ‰í† ë¦¬ëŠ” ì•„ë˜ì™€ ê°™ì€ êµ¬ì¡°ë¥¼ ê°€ì§„ë‹¤.\n",
    "\n",
    "```\n",
    "poetry-demo\n",
    "â”œâ”€â”€ pyproject.toml\n",
    "â”œâ”€â”€ README.rst\n",
    "â”œâ”€â”€ poetry_demo\n",
    "â”‚   â””â”€â”€ __init__.py\n",
    "â””â”€â”€ tests\n",
    "    â”œâ”€â”€ __init__.py\n",
    "    â””â”€â”€ test_poetry_demo.py\n",
    "```\n",
    "\n",
    "**âœ”ï¸ `pyproject.toml`**\n",
    "toml íŒŒì¼ì€ ì¤‘ìš” ìš”ì†Œ ì¤‘ í•˜ë‚˜ì¸ë°, í”„ë¡œì íŠ¸ì˜ ì˜ì¡´ì„±ì„ ì¡°ìœ¨í•´ ì£¼ëŠ” íŒŒì¼ì´ê¸° ë•Œë¬¸.\n",
    "`pyproject.toml` íŒŒì¼ì„ ë³€ê²½í•  í•„ìš” ì—†ì´ ì•„ë˜ commandë¥¼ í†µí•´ ìˆ˜ì •ì´ ê°€ëŠ¥. \n",
    "\n",
    "```bash\n",
    "poetry add some-dependencies-you-want\n",
    "```\n",
    "\n",
    "`poetry.lock`\n",
    "í•­ìƒ ê°™ì€ ì˜ì¡´ì„± í™˜ê²½ì—ì„œ ê°œë°œí•  ìˆ˜ ìˆë„ë¡ ì˜ì¡´ì„± version locked.\n",
    "ì²˜ìŒ í”„ë¡œì íŠ¸ êµ¬ì„± ì‹œ, í”„ë¡œì íŠ¸ì— ì •ì˜ëœ ì˜ì¡´ì„± íŒŒì¼ë“¤ì„ ì„¤ì¹˜ë¥¼ ìœ„í•œ ëª…ë ¹ì–´.\n",
    "\n",
    "```\n",
    "poetry install\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "## Build a ChatBot Application!\n",
    "\n",
    "### Install Dependencies\n",
    "\n",
    "```\n",
    "poetry add langchain openai pinecone-client langchain-pinecone langchain-openai python-dotenv pypdf\n",
    "```\n",
    "\n",
    "### Set Environment Variables\n",
    "\n",
    "Create a file named `.env` in your project directory. Remember to keep this file safe â€” itâ€™ll hold sensitive information like your API keys. Weâ€™ll use a Python library called dotenv to access these keys securely.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY={openAI API key}\n",
    "INDEX_NAME={Pinecone index name}\n",
    "PINECONE_API_KEY={Pinecone API key}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "load_dotenv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
