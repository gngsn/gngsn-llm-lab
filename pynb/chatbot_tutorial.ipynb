{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Tutorial\n",
    "\n",
    "\n",
    "## Concept.\n",
    "\n",
    "### LLM\n",
    "Large Language Models are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, question answering, and more, without needing task-specific fine tuning for every scenario.\n",
    "\n",
    "Modern LLMs are typically accessed through a chat model interface that takes a list of [messages](https://python.langchain.com/docs/concepts/messages/) as input and returns a message as output.\n",
    "\n",
    "\n",
    "### RAG\n",
    "Retrieval-Augmented Generation. \n",
    "It’s like giving your chatbot a brain full of **searchable knowledge**.\n",
    "Imagine a chatbot that could tap into a vast library of information and generate creative text. That’s the magic of RAG.\n",
    "\n",
    "### LangChain\n",
    "\n",
    "LangChain is a powerful Python-based framework designed to simplify the development of applications powered by large language models (LLMs). It does this by providing a modular and flexible structure that streamlines common NLP tasks and the integration of various AI components.\n",
    "\n",
    "#### Key Players\n",
    "\n",
    "- `Chains`: These are essentially pipelines that connect various components within the chatbot architecture. In our case, we’ll construct a chain that seamlessly integrates the user’s query with the retrieval manager (RAG) and subsequently, the response generation stage.\n",
    "- `Agents`: These are like the workers in your chatbot factory. We will create a specialized agent that acts as a bridge between LangChain and RAG. It receives the user’s query from the chain, transmits it to RAG for information retrieval, and then feeds the retrieved data back into the chain for further processing.\n",
    "- `Prompts`: LangChain empowers you to design prompts that guide the LLM in crafting the most effective response. For example, a prompt might instruct the LLM to provide a succinct answer to the user’s question or generate a concise summary of a retrieved Wikipedia article.\n",
    "\n",
    "\n",
    "#### LifeCycle\n",
    "\n",
    "- `Development`: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\n",
    "- `Productionization`: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\n",
    "- `Deployment`: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\n",
    "\n",
    "\n",
    "#### 🤝🏻 RAG\n",
    "The synergy between RAG’s information retrieval capabilities and LangChain’s modular structure lays the foundation for constructing a chatbot that leverages the vast and complicated content in any data source (in our case PDF) to deliver informative and creative responses to user queries.\n",
    "\n",
    "\n",
    "### ChatModels\n",
    "\n",
    "[docs](https://python.langchain.com/docs/concepts/chat_models/)\n",
    "Chat models offer a standard set of parameters that can be used to configure the model. These parameters are typically used to control the behavior of the model, such as the temperature of the output, the maximum number of tokens in the response, and the maximum time to wait for a response. Please see the standard parameters section for more details.\n",
    "\n",
    "\n",
    "#### Interface\n",
    "\n",
    "[BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html)\n",
    "\n",
    "**Key methods**\n",
    "\n",
    "- `invoke`: The primary method for interacting with a chat model. It takes a list of messages as input and returns a list of messages as output.\n",
    "- `stream`: A method that allows you to stream the output of a chat model as it is generated.\n",
    "- `batch`: A method that allows you to batch multiple requests to a chat model together for more efficient processing.\n",
    "- `bind_tools`: A method that allows you to bind a tool to a chat model for use in the model's execution context.\n",
    "- `with_structured_output`: A wrapper around the invoke method for models that natively support structured output.\n",
    "\n",
    "\n",
    "\n",
    "## 1. Setting Environment \n",
    "\n",
    "### 1.1 `pipx` installation\n",
    "### 1.2 `poetry` installation\n",
    "\n",
    "`poetry` 명령어가 인식되지 않는다면, poetry 설치 경로 `$PATH` 에 추가\n",
    "\n",
    "```bash\n",
    "export PATH=\"$HOME/.local/bin:$PATH\"\n",
    "``` \n",
    "\n",
    "## 2. API Keys\n",
    "\n",
    "Visit '[Get a Gemini API key](https://ai.google.dev/gemini-api/docs/api-key)' to create an account if you don’t have one.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 3. create a poetry project\n",
    "\n",
    "poetry를 사용한 프로젝트 생성 명령어는 다음과 같다.\n",
    "\n",
    "```bash\n",
    "poetry new poetry-demo\n",
    "```\n",
    "이 명령어는 poetry-demo라는 디렉토리를 만들어주고, 해당 디렉토리는 아래와 같은 구조를 가진다.\n",
    "\n",
    "```\n",
    "poetry-demo\n",
    "├── pyproject.toml\n",
    "├── README.rst\n",
    "├── poetry_demo\n",
    "│   └── __init__.py\n",
    "└── tests\n",
    "    ├── __init__.py\n",
    "    └── test_poetry_demo.py\n",
    "```\n",
    "\n",
    "**✔️ `pyproject.toml`**\n",
    "toml 파일은 중요 요소 중 하나인데, 프로젝트의 의존성을 조율해 주는 파일이기 때문.\n",
    "`pyproject.toml` 파일을 변경할 필요 없이 아래 command를 통해 수정이 가능. \n",
    "\n",
    "```bash\n",
    "poetry add some-dependencies-you-want\n",
    "```\n",
    "\n",
    "`poetry.lock`\n",
    "항상 같은 의존성 환경에서 개발할 수 있도록 의존성 version locked.\n",
    "처음 프로젝트 구성 시, 프로젝트에 정의된 의존성 파일들을 설치를 위한 명령어.\n",
    "\n",
    "```\n",
    "poetry install\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "## Build a ChatBot Application!\n",
    "\n",
    "### Install Dependencies\n",
    "\n",
    "```\n",
    "poetry add langchain openai pinecone-client langchain-pinecone langchain-openai python-dotenv pypdf\n",
    "```\n",
    "\n",
    "### Set Environment Variables\n",
    "\n",
    "Create a file named `.env` in your project directory. Remember to keep this file safe — it’ll hold sensitive information like your API keys. We’ll use a Python library called dotenv to access these keys securely.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY={openAI API key}\n",
    "INDEX_NAME={Pinecone index name}\n",
    "PINECONE_API_KEY={Pinecone API key}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2. Loading the PDF Content\n",
    "\n",
    "[langchain - document_loaders](https://python.langchain.com/docs/integrations/document_loaders/pypdfloader/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "def get_root_dir():\n",
    "    return os.path.dirname(os.getcwd())\n",
    "\n",
    "root_dir = get_root_dir()\n",
    "loader = PyPDFLoader(root_dir + \"/data/오가노이드사이언스_증권신고서.pdf\")\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pp(document[0].metadata)\n",
    "print(document[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3. Splitting Documents into Chunks\n",
    "\n",
    "**Understanding Chunk Size and Overlap**\n",
    "\n",
    "The CharacterTextSplitter takes two important parameters:\n",
    "\n",
    "- `chunk_size`: This defines the maximum number of characters in each split chunk. A smaller chunk size creates more, but shorter, chunks. A larger chunk size creates fewer, but longer, chunks. It’s crucial to find a good balance when choosing the chunk_size. Extremely small chunks might not contain enough context for the language model to understand properly, whereas very large chunks might become processing bottlenecks.\n",
    "\n",
    "- `chunk_overlap`: This specifies the number of characters that overlap between consecutive chunks. Overlap helps the language model maintain context between neighboring pieces, especially important for longer sentences that might be split across chunks. We set chunk_overlap to 100 characters. This ensures some overlap between chunks, helping the language model understand the flow of information across splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(document)\n",
    "print(f\"created {len(texts)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding models\n",
    "\n",
    "[LangChain - Embedding models](https://python.langchain.com/docs/concepts/embedding_models/)\n",
    "\n",
    "<img src='./img/figure1-1.png' />\n",
    "\n",
    "- (1) **Embed text as a vector**: Embeddings transform text into a numerical vector representation.\n",
    "- (2) **Measure similarity**: Embedding vectors can be compared using simple mathematical operations.\n",
    "\n",
    "[AI-Powered (Vector) Search](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "embeddings = VertexAIEmbeddings(model=\"text-embedding-004\")\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Initializing OpenAI**: We initialize the ChatOpenAI LLM specifying temperature=0. This encourages the model to generate more focused and less creative responses.\n",
    "- **Core RAG Chain**: In LangChain, RetrievalQA.from_chain_type is a function used to create a RetrievalQA chain, a specific type of chain designed for question answering tasks.\n",
    "\n",
    "\n",
    "If you want customized answers but no training, use RAG.\n",
    "If you want a chatbot that improves over time, use RLHF.\n",
    "If you want full customization, fine-tune your own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "chat = init_chat_model(\"gemini-2.0-flash-001\", model_provider=\"google_vertexai\")\n",
    "# chat = ChatOpenAI(verbose=True, temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=chat, chain_type=\"stuff\", retriever=vector_store.as_retriever()\n",
    ")    \n",
    "\n",
    "res = qa.invoke(\"What are the applications of generative AI according the the paper? Please number each application.\")\n",
    "print(res) \n",
    "\n",
    "res = qa.invoke(\"Can you please elaborate more on application number 2?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the document referring to? Please summarize the whole content.\n",
      "Bot: This document outlines regulations and potential restrictions related to participating in the demand forecast for a public offering (IPO).\n",
      "\n",
      "Here's a summary of the key points:\n",
      "\n",
      "*   **Restrictions on Participation:** Certain entities are restricted from participating in the demand forecast, including those with conflicts of interest, major shareholders of companies recently underwritten by the lead manager, those currently classified as \"unfaithful demand forecast participants,\" and certain venture capital firms.\n",
      "*   **\"Unfaithful Demand Forecast Participant\" Designation:** If an institution is designated as an \"unfaithful demand forecast participant\" after participating in the current demand forecast, they will be restricted from participating in future demand forecasts and from being allocated IPO shares for a certain period.\n",
      "*   **Regulations on IPO Pricing:** The document references regulations regarding how the IPO price is determined, including the possibility of allowing certain venture capital firms to participate in the demand forecast.\n",
      "*   **Put Option (환매청구권):** If certain venture capital firms are allowed to participate in the demand forecast, general investors may be granted the right to sell their shares back to the underwriter under certain conditions.\n",
      "*   **Disqualification as \"Unfaithful Demand Forecast Participant\":** The document warns that institutions may be classified as \"unfaithful demand forecast participants\" based on specific reasons outlined in the \"Securities Underwriting Business Regulations.\" Institutions classified as \"unfaithful demand forecast participants\" are restricted from participating in future demand forecasts led by Korea Investment & Securities.\n"
     ]
    }
   ],
   "source": [
    "### 🤖 Step 3: Create a Simple Chatbot\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Create chatbot instance\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "# from langchain.chains import ConversationChain\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "import pprint\n",
    "\n",
    "# Add memory to chatbot\n",
    "chat = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "# Load pdf file\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "loader = PyPDFLoader(root_dir + \"/data/오가노이드사이언스_증권신고서.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# pprint.pp(document[0].metadata)\n",
    "# print(document[0].page_content[:1000])\n",
    "\n",
    "# Convert text into embeddings\n",
    "vectorstore = Chroma.from_documents(documents, GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\"))\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "# chatbot = ConversationChain(llm=chat, memory=memory)\n",
    "qa = RetrievalQA.from_chain_type(llm=chat, retriever=retriever)\n",
    "\n",
    "\n",
    "print(\"Question: What is the document referring to? Please summarize the whole content.\")\n",
    "response = qa.run(\"What is the document referring to? Please summarize the whole content\")\n",
    "print(\"Bot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n",
      "['', 'Users']\n",
      "['', 'Users', 'gyeongsun']\n",
      "['', 'Users', 'gyeongsun', 'git']\n",
      "['', 'Users', 'gyeongsun', 'git', 'gngsn-llm-lab']\n",
      "/Users/gyeongsun/git/gngsn-llm-lab\n"
     ]
    }
   ],
   "source": [
    "def get_working_dir():\n",
    "    working_dir = \"gngsn-llm-lab\"\n",
    "    root = []\n",
    "    for p in os.getcwd().split(\"/\"):\n",
    "        root.append(p)\n",
    "        print(root)\n",
    "        if (p == working_dir): \n",
    "            break\n",
    "\n",
    "    print(\"/\".join(root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
